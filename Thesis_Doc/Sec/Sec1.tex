\documentclass[../thesis_main.tex]{subfiles}

\begin{document}
\doublespacing
\section{Introduction}

Grades represent one of the most widely used methods for evaluating student achievement in K-12 education. Not only are they a near constantly considered outcome in K-12 schools, prior research has shown that GPA is a powerful predictor of long-term student success (\citealt{allensworthHighSchoolGPAs2020, eastonPredictivePowerNinthGrade2017}). Nevertheless, there remains tension around the use of grades as a key metric by which to evaluate student achievement, with some education professionals and policymakers concerned that grading practices and the standards of success could vary wildly between teachers or schools (\citealt{gershensonGradeInflationHigh2018}). If true, then grades are an at-best unreliable indicator of student learning, and caution should be exercised in the use of grades as an evaluative tool. 

Due to this tension, there is great policy relevance in understanding the extent to which observed variation in student grades is a function of underlying variation in teacher grading practices not directly related to student learning. The ability to identify and controll for these teacher-level effects would increase the reliability of students' grades as both a measure of achievement and as a predictor of long-term success. While this alone would represent an important contribution to the conversation around using GPAs as a primary measure of student performance, the measurement of these grading tendencies would also provide a framework with which to examine the consequences of the underlying heterogeneity in teacher grading practices itself.

Think, for example, of an ``easy-A'' teacher versus a ``tough grader''. While both teachers may grade consistely within their own classroom, an individual student's report card could reflect very different outcomes when assigned to one teacher or the other, even if they reach the same level of understanding of the material. At the individual level, this might artifically raise or lower one student's GPA relative to their peers. This could, in turn, alter that student's later course taking patterns, test scores, or even their odds of enrolling in college. At the aggregate level, sorting of students with a given background to teachears with a particular grading practice could result in this underlying heterogeneity multiplying the single student effect and widening pre-existing inequities in both student grades and other, long-term outcomes. 

The challenge arises, however, is parsing the extent to which this grading effect is truly idiosyncratic, rather than capturing other non-observable determinants of student achievement (such as attitude or engagement) which themselves might go beyond mastery of the content.  To attempt to answer this questions, we turn to Chicago Public Schools (CPS). With more than 20,000 teachers and 350,000 students, distributed over 500 schools, CPS provides a setting of both adequate variation among schools, teachers, and students, within a homogeneous district-wide setting to allow for productive empirical analysis. In a previous analysis, \citet{allensworthWhyStudentsGet2018} found that even after adding numerous controls for observable determinants of student grades, a significant portion of between-teacher grading variation remained. The richness and size of the data allow us to estimate and quasi-experimentally evaluate idiosyncratic teacher grading practices, allowing for the underlying causes of this remaining variation to be examined in a new light. 

In the initial step of the analysis, I develop a strategy to identify the idiosyncratic grading practices of individual teachers. Specifically, I develop a theoretical argument to identify the \textit{teacher grading effect}, as the component of a teacher's students' grades that is not linked to increases in human capital. An important note: this argument does not explain the reasons for the existence of this effect, nor does it try. This could either be ``grade inflation'' in the colloquial sense, or it could be related to other aspects of the student-teacher relationship which, while not connected with the actual learning of the material, could still very much be due to circumstances within the classroom. 

I next provide a statistical methodology to estimate these effects using longitudinal CPS data, using a sample of CPS freshman from school years 2010-2011 to 2013-2014. For every mathematics and English teacher in this sample, I then calculate their individual teacher grading effect using a two-step procedure, using a combination of OLS fixed-effects models and empirical Bayes estimation procedures to generate numerical estimates of these scores. I then use these estimates in a variety of model specifications to analyze the relationship between observed variation in teacher grading effects and residual variation in high school course taking patterns, as well as academic and post-secondary outcomes. 

%These include the average teacher grading effect in core courses during a student's freshman year, relative measures of the teacher grading effect of a student's freshman year teachers as compared to their within-school peers, and indicators of whether or not a student had an unusually high positive or negative grading effect teacher at any point during their freshman year.

Building on \citet{gilraineMakingTeachingLast2020}, my identification argument for the teacher grading effects rests on three primary assumptions: (1) as-good-as-random assignment of teachers in freshman year, (2) as-good-as-random assignment of teachers in sophomore year, and (3) a functional assumption on the human capital accumulation process. Thus, in addition to my primary analysis, I also conduct several tests to attempt to provide additional evidence for each of these assumptions. For the random assignment assumption, I run a panel of balance tests on observable teacher and student characteristics for both freshman and sophomore year teachers to attempt to identify any sorting based on student or prior teacher characteristics. For the functional form assumption, I run a specification test examining the degree to which my results are robust to variation in the depreciation of human capital. Finally, to test the overall validity of the theoretic framework, I also run a ``movers'' design - effectively an event study on teacher switching within CPS - to examine the extent to which estimated teacher grading effects can predict real changes in student grades under quasi-experimental variation. 


\end{document}